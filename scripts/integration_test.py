#!/usr/bin/env python3
"""
LQG-ANEC Framework Integration Test Script

Comprehensive integration test that validates all modules and demonstrates
the complete workflow from quantum geometry to ANEC violation detection.

Key Features:
- Tests all framework modules: custom kernels, ghost-condensate EFT, semi-classical LQG, backreaction
- Validates breakthrough results and documented discoveries
- Generates comprehensive analysis reports
- Benchmarks GPU performance and computational efficiency
- Produces publication-ready visualizations

Integration Workflow:
1. Custom kernel library validation and QI bound scanning
2. Ghost-condensate EFT ANEC violation tests
3. Semi-classical LQG stress-tensor computation
4. Backreaction and geometry stability analysis
5. Cross-module consistency validation
6. Performance benchmarking and scaling analysis
"""

import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import torch
import matplotlib.pyplot as plt

# Add src to path
framework_root = Path(__file__).parent.parent
sys.path.append(str(framework_root / "src"))

# Import framework modules
try:
    from custom_kernels import CustomKernelLibrary, create_standard_library
    from ghost_condensate_eft import GhostCondensateEFT, GhostEFTParameters
    from semi_classical_stress import SemiClassicalStressTensor, LQGParameters
    
    # Import existing analysis scripts
    sys.path.append(str(framework_root / "scripts"))
    from test_backreaction import run_backreaction_analysis
    
except ImportError as e:
    print(f"Error importing framework modules: {e}")
    print("Please ensure all modules are properly installed.")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(framework_root / "integration_test.log")
    ]
)
logger = logging.getLogger(__name__)


class FrameworkIntegrationTester:
    """
    Comprehensive integration tester for LQG-ANEC framework.
    
    Tests all modules, validates breakthrough results, and generates
    comprehensive analysis reports with performance benchmarking.
    """
    
    def __init__(self, output_dir: str = "results/integration_test"):
        """Initialize integration tester."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.test_results = {}
        self.performance_metrics = {}
        
        logger.info(f"Framework integration tester initialized")
        logger.info(f"Device: {self.device}")
        logger.info(f"Output directory: {self.output_dir}")
    
    def test_custom_kernels(self) -> Dict[str, Any]:
        """Test custom kernel library and QI bound scanning."""
        logger.info("Testing custom kernel library...")
        
        start_time = time.time()
        
        # Create kernel library
        lib = create_standard_library()
        
        # Test on multiple time scales
        tau0_vals = np.logspace(-2, 3, 15)  # Reduced for integration test
        
        # Run kernel tests
        logger.info(f"Testing {len(lib.kernels)} kernels on {len(tau0_vals)} time scales...")
        results = lib.test_kernels(tau0_vals)
        
        # Find violations
        violations = lib.find_violations(tau0_vals, violation_threshold=-1e-12)
        
        # Get best performers
        best_performers = lib.generate_best_performers(tau0_vals, top_n=5)
        
        # Performance metrics
        test_time = time.time() - start_time
        
        # Visualizations
        try:
            lib.visualize_kernels(save_path=str(self.output_dir / "custom_kernels_overview.png"))
            lib.visualize_bounds(tau0_vals, save_path=str(self.output_dir / "kernel_bounds.png"))
        except Exception as e:
            logger.warning(f"Visualization failed: {e}")
          # Summary statistics
        total_violations = sum([sum(viols) for viols in violations.values()])
        total_tests = sum([len(viols) for viols in violations.values()])
        
        test_summary = {
            'n_kernels': len(lib.kernels),
            'n_time_scales': len(tau0_vals),
            'total_violations': total_violations,
            'total_tests': total_tests,
            'violation_rate': total_violations / total_tests if total_tests > 0 else 0,
            'best_performers': best_performers,
            'test_time': test_time,
            'kernels_per_second': len(lib.kernels) / test_time
        }
        
        logger.info(f"Custom kernels test complete: {total_violations}/{total_tests} violations "
                   f"({test_summary['violation_rate']:.1%}) in {test_time:.2f}s")\n        \n        return test_summary\n    \n    def test_ghost_condensate_eft(self) -> Dict[str, Any]:\n        \"\"\"Test ghost-condensate EFT module.\"\"\"\n        logger.info(\"Testing ghost-condensate EFT...\")\n        \n        start_time = time.time()\n        \n        # Create EFT parameters\n        params = GhostEFTParameters(\n            phi_0=1.0,\n            lambda_ghost=0.1,\n            cutoff_scale=10.0,\n            grid_size=32,  # Smaller for integration test\n            device=str(self.device)\n        )\n        \n        # Initialize EFT system\n        ghost_eft = GhostCondensateEFT(params)\n        \n        # Compute stress-energy tensor\n        stress_tensor = ghost_eft.stress_energy_tensor()\n        \n        # Compute ANEC integral\n        anec_result = ghost_eft.compute_anec_integral()\n        \n        # Stability analysis\n        stability = ghost_eft.stability_analysis()\n        \n        # Performance metrics\n        test_time = time.time() - start_time\n        \n        # Summary statistics\n        test_summary = {\n            'grid_size': params.grid_size,\n            'anec_total': float(anec_result['anec_total']),\n            'anec_violation': anec_result['anec_total'] < 0,\n            'negative_energy_fraction': float(stability['negative_energy_fraction']),\n            'total_energy': float(stability['total_energy']),\n            'energy_variance': float(stability['energy_variance']),\n            'test_time': test_time,\n            'grid_points_per_second': params.grid_size**3 / test_time\n        }\n        \n        logger.info(f\"Ghost EFT test complete: ANEC = {test_summary['anec_total']:.2e}, \"\n                   f\"Violation = {test_summary['anec_violation']}, Time = {test_time:.2f}s\")\n        \n        return test_summary\n    \n    def test_semi_classical_lqg(self) -> Dict[str, Any]:\n        \"\"\"Test semi-classical LQG stress-tensor module.\"\"\"\n        logger.info(\"Testing semi-classical LQG stress-tensor...\")\n        \n        start_time = time.time()\n        \n        # Create LQG parameters\n        params = LQGParameters(\n            polymer_scale=1.0,\n            polymer_boost=1.5,\n            coherent_scale=1000.0,\n            grid_resolution=32,  # Smaller for integration test\n            device=str(self.device)\n        )\n        \n        # Initialize LQG stress system\n        lqg_stress = SemiClassicalStressTensor(params)\n        \n        # Compute stress-tensor components\n        stress_components = lqg_stress.compute_stress_tensor_expectation()\n        \n        # Compute ANEC integral\n        anec_result = lqg_stress.compute_anec_integral()\n        \n        # Quantum correction analysis\n        corrections = lqg_stress.analyze_quantum_corrections()\n        \n        # Performance metrics\n        test_time = time.time() - start_time\n        \n        # Summary statistics\n        test_summary = {\n            'grid_resolution': params.grid_resolution,\n            'n_nodes': len(lqg_stress.nodes),\n            'n_edges': len(lqg_stress.edges),\n            'anec_total': float(anec_result['anec_total']),\n            'anec_violation': anec_result['anec_total'] < 0,\n            'polymer_enhancement': float(corrections['polymer_enhancement_factor']),\n            'quantum_correction_fraction': float(corrections['quantum_correction_fraction']),\n            'test_time': test_time,\n            'nodes_per_second': len(lqg_stress.nodes) / test_time if test_time > 0 else 0\n        }\n        \n        logger.info(f\"LQG stress test complete: {len(lqg_stress.nodes)} nodes, \"\n                   f\"ANEC = {test_summary['anec_total']:.2e}, \"\n                   f\"Polymer boost = {test_summary['polymer_enhancement']:.3f}\")\n        \n        return test_summary\n    \n    def test_backreaction_analysis(self) -> Dict[str, Any]:\n        \"\"\"Test backreaction and geometry stability analysis.\"\"\"\n        logger.info(\"Testing backreaction analysis...\")\n        \n        start_time = time.time()\n        \n        try:\n            # Run backreaction analysis with moderate parameters\n            result = run_backreaction_analysis(\n                energy_scale=-1e-11,\n                grid_size=24,  # Smaller for integration test\n                evolution_steps=100,  # Fewer steps\n                device=str(self.device)\n            )\n            \n            test_time = time.time() - start_time\n            \n            # Extract key metrics\n            test_summary = {\n                'energy_scale': result['parameters']['energy_scale'],\n                'grid_size': result['parameters']['grid_size'],\n                'evolution_steps': result['parameters']['evolution_steps'],\n                'max_metric_deformation': result['evolution_results']['max_metric_deformation'],\n                'final_metric_deformation': result['evolution_results']['final_metric_deformation'],\n                'is_stable': result['stability_results']['is_stable'],\n                'max_growth_rate': result['stability_results']['max_growth_rate'],\n                'anec_violations': result['anec_results'].get('violation_duration', 0),\n                'test_time': test_time,\n                'steps_per_second': result['parameters']['evolution_steps'] / test_time\n            }\n            \n            logger.info(f\"Backreaction test complete: Deformation = {test_summary['max_metric_deformation']:.2e}, \"\n                       f\"{'STABLE' if test_summary['is_stable'] else 'UNSTABLE'}, \"\n                       f\"{test_summary['anec_violations']} ANEC violations\")\n            \n        except Exception as e:\n            logger.error(f\"Backreaction test failed: {e}\")\n            test_summary = {\n                'error': str(e),\n                'test_time': time.time() - start_time,\n                'success': False\n            }\n        \n        return test_summary\n    \n    def validate_breakthrough_results(self) -> Dict[str, Any]:\n        \"\"\"Validate key breakthrough results from documentation.\"\"\"\n        logger.info(\"Validating breakthrough results...\")\n        \n        validation_results = {\n            'polymer_enhancement_validated': False,\n            'ghost_eft_violation_validated': False,\n            'lqg_stress_enhancement_validated': False,\n            'week_scale_optimization_validated': False\n        }\n        \n        # Test polymer enhancement formula validation\n        try:\n            lib = create_standard_library()\n            tau0_vals = np.array([604800.0])  # Week scale\n            results = lib.test_kernels(tau0_vals)\n            \n            week_scale_bound = results.get('week_scale', [np.nan])[0]\n            polymer_week_bound = results.get('polymer_week', [np.nan])[0]\n            \n            # Validate polymer enhancement (should show improvement)\n            if not np.isnan(week_scale_bound) and not np.isnan(polymer_week_bound):\n                enhancement_factor = abs(polymer_week_bound) / abs(week_scale_bound)\n                validation_results['polymer_enhancement_validated'] = enhancement_factor > 1.0\n                validation_results['polymer_enhancement_factor'] = enhancement_factor\n            \n        except Exception as e:\n            logger.warning(f\"Polymer enhancement validation failed: {e}\")\n        \n        # Test ghost EFT violation\n        try:\n            params = GhostEFTParameters(grid_size=16, device=str(self.device))\n            ghost_eft = GhostCondensateEFT(params)\n            anec_result = ghost_eft.compute_anec_integral()\n            \n            validation_results['ghost_eft_violation_validated'] = anec_result['anec_total'] < 0\n            validation_results['ghost_eft_anec_value'] = float(anec_result['anec_total'])\n            \n        except Exception as e:\n            logger.warning(f\"Ghost EFT validation failed: {e}\")\n        \n        # Test week-scale optimization\n        try:\n            lib = create_standard_library()\n            tau0_vals = np.logspace(4, 6, 5)  # Around week scale\n            results = lib.test_kernels(tau0_vals)\n            \n            week_results = results.get('week_scale', [])\n            if week_results:\n                min_week_bound = min([abs(b) for b in week_results if not np.isnan(b)])\n                validation_results['week_scale_optimization_validated'] = min_week_bound > 1e-12\n                validation_results['week_scale_min_bound'] = min_week_bound\n            \n        except Exception as e:\n            logger.warning(f\"Week-scale validation failed: {e}\")\n        \n        validated_count = sum([v for k, v in validation_results.items() if k.endswith('_validated')])\n        validation_results['total_validations'] = validated_count\n        validation_results['validation_success_rate'] = validated_count / 4\n        \n        logger.info(f\"Breakthrough validation: {validated_count}/4 results validated \"\n                   f\"({validation_results['validation_success_rate']:.1%})\")\n        \n        return validation_results\n    \n    def performance_benchmark(self) -> Dict[str, Any]:\n        \"\"\"Run performance benchmarks across all modules.\"\"\"\n        logger.info(\"Running performance benchmarks...\")\n        \n        benchmarks = {}\n        \n        # Custom kernels benchmark\n        start_time = time.time()\n        lib = create_standard_library()\n        tau0_vals = np.logspace(-1, 2, 10)\n        lib.test_kernels(tau0_vals)\n        benchmarks['custom_kernels_time'] = time.time() - start_time\n        benchmarks['custom_kernels_throughput'] = len(lib.kernels) * len(tau0_vals) / benchmarks['custom_kernels_time']\n        \n        # GPU memory benchmark\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats()\n            \n            # Ghost EFT benchmark\n            start_time = time.time()\n            params = GhostEFTParameters(grid_size=48, device=str(self.device))\n            ghost_eft = GhostCondensateEFT(params)\n            ghost_eft.compute_anec_integral()\n            benchmarks['ghost_eft_time'] = time.time() - start_time\n            benchmarks['ghost_eft_memory_gb'] = torch.cuda.max_memory_allocated() / 1e9\n            \n            torch.cuda.reset_peak_memory_stats()\n            \n        # Overall throughput metrics\n        total_time = sum([v for k, v in benchmarks.items() if k.endswith('_time')])\n        benchmarks['total_benchmark_time'] = total_time\n        benchmarks['device_type'] = str(self.device)\n        benchmarks['cuda_available'] = torch.cuda.is_available()\n        \n        if torch.cuda.is_available():\n            benchmarks['gpu_name'] = torch.cuda.get_device_name()\n            benchmarks['gpu_memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        \n        logger.info(f\"Performance benchmark complete: Total time = {total_time:.2f}s\")\n        \n        return benchmarks\n    \n    def generate_integration_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive integration test report.\"\"\"\n        logger.info(\"Generating integration test report...\")\n        \n        # Run all tests\n        start_time = time.time()\n        \n        try:\n            kernel_results = self.test_custom_kernels()\n        except Exception as e:\n            logger.error(f\"Custom kernels test failed: {e}\")\n            kernel_results = {'error': str(e), 'success': False}\n        \n        try:\n            ghost_results = self.test_ghost_condensate_eft()\n        except Exception as e:\n            logger.error(f\"Ghost EFT test failed: {e}\")\n            ghost_results = {'error': str(e), 'success': False}\n        \n        try:\n            lqg_results = self.test_semi_classical_lqg()\n        except Exception as e:\n            logger.error(f\"LQG stress test failed: {e}\")\n            lqg_results = {'error': str(e), 'success': False}\n        \n        try:\n            backreaction_results = self.test_backreaction_analysis()\n        except Exception as e:\n            logger.error(f\"Backreaction test failed: {e}\")\n            backreaction_results = {'error': str(e), 'success': False}\n        \n        try:\n            validation_results = self.validate_breakthrough_results()\n        except Exception as e:\n            logger.error(f\"Breakthrough validation failed: {e}\")\n            validation_results = {'error': str(e), 'success': False}\n        \n        try:\n            performance_results = self.performance_benchmark()\n        except Exception as e:\n            logger.error(f\"Performance benchmark failed: {e}\")\n            performance_results = {'error': str(e), 'success': False}\n        \n        total_time = time.time() - start_time\n        \n        # Compile comprehensive report\n        integration_report = {\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'framework_version': '1.0.0',\n            'device': str(self.device),\n            'total_test_time': total_time,\n            \n            'module_tests': {\n                'custom_kernels': kernel_results,\n                'ghost_condensate_eft': ghost_results,\n                'semi_classical_lqg': lqg_results,\n                'backreaction_analysis': backreaction_results\n            },\n            \n            'breakthrough_validation': validation_results,\n            'performance_metrics': performance_results,\n            \n            'summary': {\n                'modules_tested': 4,\n                'modules_successful': sum([1 for r in [kernel_results, ghost_results, lqg_results, backreaction_results] \n                                          if not r.get('error')]),\n                'breakthrough_validations': validation_results.get('total_validations', 0),\n                'overall_success': True  # Will be updated based on results\n            }\n        }\n        \n        # Update overall success\n        integration_report['summary']['overall_success'] = (\n            integration_report['summary']['modules_successful'] >= 3 and\n            validation_results.get('total_validations', 0) >= 2\n        )\n        \n        # Save report\n        report_file = self.output_dir / \"integration_test_report.json\"\n        with open(report_file, 'w') as f:\n            json.dump(integration_report, f, indent=2, default=str)\n        \n        logger.info(f\"Integration test report saved to {report_file}\")\n        logger.info(f\"Overall success: {integration_report['summary']['overall_success']}\")\n        logger.info(f\"Modules successful: {integration_report['summary']['modules_successful']}/4\")\n        logger.info(f\"Breakthroughs validated: {integration_report['summary']['breakthrough_validations']}/4\")\n        \n        return integration_report\n    \n    def create_summary_visualization(self, report: Dict[str, Any]):\n        \"\"\"Create summary visualization of integration test results.\"\"\"\n        try:\n            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n            \n            # Module success rates\n            modules = ['Custom\\nKernels', 'Ghost\\nEFT', 'LQG\\nStress', 'Backreaction']\n            successes = [not report['module_tests'][k].get('error', False) for k in \n                        ['custom_kernels', 'ghost_condensate_eft', 'semi_classical_lqg', 'backreaction_analysis']]\n            \n            colors = ['green' if s else 'red' for s in successes]\n            axes[0, 0].bar(modules, [1 if s else 0 for s in successes], color=colors, alpha=0.7)\n            axes[0, 0].set_ylabel('Success')\n            axes[0, 0].set_title('Module Test Results')\n            axes[0, 0].set_ylim(0, 1.2)\n            \n            # Performance metrics\n            if 'performance_metrics' in report and not report['performance_metrics'].get('error'):\n                perf = report['performance_metrics']\n                times = [perf.get('custom_kernels_time', 0), perf.get('ghost_eft_time', 0)]\n                time_labels = ['Custom\\nKernels', 'Ghost\\nEFT']\n                \n                axes[0, 1].bar(time_labels, times, color='blue', alpha=0.7)\n                axes[0, 1].set_ylabel('Time (s)')\n                axes[0, 1].set_title('Performance Benchmark')\n            \n            # Breakthrough validations\n            if 'breakthrough_validation' in report and not report['breakthrough_validation'].get('error'):\n                val = report['breakthrough_validation']\n                validations = ['Polymer\\nEnhancement', 'Ghost EFT\\nViolation', 'LQG Stress\\nEnhancement', 'Week Scale\\nOptimization']\n                val_results = [val.get('polymer_enhancement_validated', False),\n                              val.get('ghost_eft_violation_validated', False),\n                              val.get('lqg_stress_enhancement_validated', False),\n                              val.get('week_scale_optimization_validated', False)]\n                \n                val_colors = ['green' if v else 'red' for v in val_results]\n                axes[1, 0].bar(validations, [1 if v else 0 for v in val_results], color=val_colors, alpha=0.7)\n                axes[1, 0].set_ylabel('Validated')\n                axes[1, 0].set_title('Breakthrough Validations')\n                axes[1, 0].set_ylim(0, 1.2)\n            \n            # Summary statistics\n            summary_text = f\"\"\"Integration Test Summary\n            \nModules Successful: {report['summary']['modules_successful']}/4\nBreakthroughs Validated: {report['summary']['breakthrough_validations']}/4\nTotal Test Time: {report['total_test_time']:.1f}s\nDevice: {report['device']}\nOverall Success: {report['summary']['overall_success']}\"\"\"\n            \n            axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n                           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n            axes[1, 1].set_xlim(0, 1)\n            axes[1, 1].set_ylim(0, 1)\n            axes[1, 1].axis('off')\n            axes[1, 1].set_title('Integration Summary')\n            \n            plt.tight_layout()\n            \n            plot_file = self.output_dir / \"integration_test_summary.png\"\n            plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n            logger.info(f\"Summary visualization saved to {plot_file}\")\n            \n            plt.show()\n            \n        except Exception as e:\n            logger.warning(f\"Summary visualization failed: {e}\")\n\n\ndef main():\n    \"\"\"Run comprehensive framework integration test.\"\"\"\n    print(\"LQG-ANEC Framework Integration Test\")\n    print(\"=\" * 50)\n    print()\n    \n    # Initialize tester\n    tester = FrameworkIntegrationTester(\"results/integration_test\")\n    \n    # Run comprehensive integration test\n    logger.info(\"Starting comprehensive framework integration test...\")\n    \n    start_time = time.time()\n    report = tester.generate_integration_report()\n    total_time = time.time() - start_time\n    \n    # Create summary visualization\n    tester.create_summary_visualization(report)\n    \n    # Print results summary\n    print()\n    print(\"INTEGRATION TEST RESULTS\")\n    print(\"=\" * 50)\n    print(f\"Total test time: {total_time:.2f} seconds\")\n    print(f\"Device: {report['device']}\")\n    print()\n    \n    print(\"Module Tests:\")\n    for module, result in report['module_tests'].items():\n        status = \"✓ PASS\" if not result.get('error') else \"✗ FAIL\"\n        print(f\"  {module}: {status}\")\n        if result.get('error'):\n            print(f\"    Error: {result['error']}\")\n    \n    print()\n    print(\"Breakthrough Validations:\")\n    if not report['breakthrough_validation'].get('error'):\n        val = report['breakthrough_validation']\n        validations = [\n            ('Polymer Enhancement', val.get('polymer_enhancement_validated', False)),\n            ('Ghost EFT Violation', val.get('ghost_eft_violation_validated', False)),\n            ('LQG Stress Enhancement', val.get('lqg_stress_enhancement_validated', False)),\n            ('Week Scale Optimization', val.get('week_scale_optimization_validated', False))\n        ]\n        \n        for name, validated in validations:\n            status = \"✓ VALIDATED\" if validated else \"✗ NOT VALIDATED\"\n            print(f\"  {name}: {status}\")\n    else:\n        print(f\"  Validation failed: {report['breakthrough_validation']['error']}\")\n    \n    print()\n    print(\"Performance Metrics:\")\n    if not report['performance_metrics'].get('error'):\n        perf = report['performance_metrics']\n        print(f\"  Custom Kernels: {perf.get('custom_kernels_time', 0):.2f}s\")\n        print(f\"  Ghost EFT: {perf.get('ghost_eft_time', 0):.2f}s\")\n        if perf.get('ghost_eft_memory_gb'):\n            print(f\"  GPU Memory Used: {perf['ghost_eft_memory_gb']:.2f} GB\")\n    \n    print()\n    print(\"OVERALL RESULT:\")\n    if report['summary']['overall_success']:\n        print(\"✓ INTEGRATION TEST PASSED\")\n        print(\"  Framework is ready for research and production use.\")\n    else:\n        print(\"✗ INTEGRATION TEST FAILED\")\n        print(\"  Please review module errors and breakthrough validations.\")\n    \n    print()\n    print(f\"Detailed report saved to: {tester.output_dir / 'integration_test_report.json'}\")\n    print(f\"Summary visualization: {tester.output_dir / 'integration_test_summary.png'}\")\n    \n    return report['summary']['overall_success']\n\n\nif __name__ == \"__main__\":\n    success = main()\n    sys.exit(0 if success else 1)\n
